{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59a82135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import csv\n",
    "import numpy as np\n",
    "import lightning as L\n",
    "import data_utils.augmentations as augs\n",
    "from data_utils.datamodules import AlignedMicroDataModule\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from alignment import alignment_utils as utils\n",
    "from alignment.AlignCCA import AlignCCA\n",
    "from models import Seq2SeqRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8758723",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13cc9406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def folds_exist(path, n_folds):\n",
    "    for i in range(n_folds):\n",
    "        f = os.path.join(path, f\"fold_data/fold_{i}.h5\")\n",
    "        if not os.path.exists(f):\n",
    "            print(\"❌ Missing:\", f)\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2068824f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/pt_decoding_data_S62.pkl'\n",
    "fold_data_path = '../data/training_data_pooled'\n",
    "patient_id = 'S14' # Training on S14's data.\n",
    "batch_size = 500\n",
    "n_folds = 20\n",
    "val_size = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e484a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filename = os.path.expanduser(data_path)\n",
    "pt_data = utils.load_pkl(data_filename)\n",
    "\n",
    "phoneme_index = 1 # All samples center around phoneme 1 onset.\n",
    "lab_type = 'phon'\n",
    "algn_type = 'phon_seq'\n",
    "tar_data, pre_data = utils.decoding_data_from_dict(pt_data, patient_id, phoneme_index,\n",
    "                                                   lab_type=lab_type,\n",
    "                                                   algn_type=algn_type)\n",
    "# tar_data = X1, y1, [y1, y2, y3], on the target patient\n",
    "# pre_data = 7 * tar_data, on the 7 non-target patients\n",
    "# pre_data is not used at all in training\n",
    "\n",
    "augmentations = [augs.time_shifting, augs.noise_jitter, augs.scaling]\n",
    "data = torch.Tensor(tar_data[0])\n",
    "labels = torch.Tensor(tar_data[1]).long().unsqueeze(1) - 1\n",
    "align_labels = torch.Tensor(tar_data[2]).long() - 1\n",
    "pool_data = [(torch.Tensor(p[0]), torch.Tensor(p[2]).long() - 1, torch.Tensor(p[2]).long() - 1) for p in pre_data] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2a9ebca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Missing: ../data/training_data_pooled/fold_data/fold_4.h5\n",
      "⚙️ Generating new folds...\n",
      "DEBUG labels.shape: torch.Size([144, 3])\n",
      "DEBUG first 5 rows of labels:\n",
      " tensor([[8, 0, 4],\n",
      "        [7, 1, 4],\n",
      "        [2, 5, 3],\n",
      "        [1, 7, 0],\n",
      "        [1, 8, 0]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m⚙️ Generating new folds...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m dm = AlignedMicroDataModule(\n\u001b[32m     11\u001b[39m     data, align_labels, align_labels, pool_data, AlignCCA,\n\u001b[32m     12\u001b[39m     batch_size=batch_size, folds=n_folds, val_size=val_size,\n\u001b[32m     13\u001b[39m     augmentations=augmentations, data_path=fold_data_path\n\u001b[32m     14\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43mdm\u001b[49m\u001b[43m.\u001b[49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/EEG大模型/BCI Code/seq2seq_RNN/nn_models/data_utils/datamodules.py:253\u001b[39m, in \u001b[36mAlignedMicroDataModule.setup\u001b[39m\u001b[34m(self, stage)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m train_data, train_labels, align_labels\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# align pooled data to current data\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;66;03m# train_data, train_labels, dim_red = (\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m#     process_aligner(train_data, train_labels, align_labels,\u001b[39;00m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m#                     self.pool_data, self.algner))\u001b[39;00m\n\u001b[32m    252\u001b[39m aug_data, aug_labels, dim_red = (\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[43mprocess_aligner\u001b[49m\u001b[43m(\u001b[49m\u001b[43maug_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maug_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maug_align_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m                    \u001b[49m\u001b[43maug_pool_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43malgner\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    256\u001b[39m \u001b[38;5;66;03m# aug_data = torch.Tensor([])\u001b[39;00m\n\u001b[32m    257\u001b[39m \u001b[38;5;66;03m# aug_labels = torch.Tensor([]).long()\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# for aug in self.augmentations:\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m#     aug_data = torch.cat((aug_data, aug(train_data)))\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;66;03m#     aug_labels = torch.cat((aug_labels, train_labels))\u001b[39;00m\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m val_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/EEG大模型/BCI Code/seq2seq_RNN/nn_models/data_utils/datamodules.py:459\u001b[39m, in \u001b[36mprocess_aligner\u001b[39m\u001b[34m(X, y, y_align, pool_data, algner, n_components)\u001b[39m\n\u001b[32m    456\u001b[39m X_tar_r = X.reshape(-\u001b[32m1\u001b[39m, X.shape[-\u001b[32m1\u001b[39m])\n\u001b[32m    458\u001b[39m \u001b[38;5;66;03m# reduce dimensionality of cross-patient data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m X_cross_dr = [\u001b[43mPCA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    460\u001b[39m               \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X_cross_r]\n\u001b[32m    462\u001b[39m \u001b[38;5;66;03m# reduce dimensionality of target data, saving dim. red. object for\u001b[39;00m\n\u001b[32m    463\u001b[39m \u001b[38;5;66;03m# test set\u001b[39;00m\n\u001b[32m    464\u001b[39m tar_dr = PCA(n_components=n_components)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bci/lib/python3.12/site-packages/sklearn/utils/_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bci/lib/python3.12/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bci/lib/python3.12/site-packages/sklearn/decomposition/_pca.py:466\u001b[39m, in \u001b[36mPCA.fit_transform\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    443\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    444\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    445\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[32m    446\u001b[39m \n\u001b[32m    447\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    464\u001b[39m \u001b[33;03m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[32m    465\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m     U, S, _, X, x_is_centered, xp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    467\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m U \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    468\u001b[39m         U = U[:, : \u001b[38;5;28mself\u001b[39m.n_components_]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bci/lib/python3.12/site-packages/sklearn/decomposition/_pca.py:503\u001b[39m, in \u001b[36mPCA._fit\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    493\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    494\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPCA with svd_solver=\u001b[39m\u001b[33m'\u001b[39m\u001b[33marpack\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is not supported for Array API inputs.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    495\u001b[39m     )\n\u001b[32m    497\u001b[39m \u001b[38;5;66;03m# Validate the data, without ever forcing a copy as any solver that\u001b[39;00m\n\u001b[32m    498\u001b[39m \u001b[38;5;66;03m# supports sparse input data and the `covariance_eigh` solver are\u001b[39;00m\n\u001b[32m    499\u001b[39m \u001b[38;5;66;03m# written in a way to avoid the need for any inplace modification of\u001b[39;00m\n\u001b[32m    500\u001b[39m \u001b[38;5;66;03m# the input data contrary to the other solvers.\u001b[39;00m\n\u001b[32m    501\u001b[39m \u001b[38;5;66;03m# The copy will happen\u001b[39;00m\n\u001b[32m    502\u001b[39m \u001b[38;5;66;03m# later, only if needed, once the solver negotiation below is done.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[38;5;28mself\u001b[39m._fit_svd_solver = \u001b[38;5;28mself\u001b[39m.svd_solver\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fit_svd_solver == \u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m issparse(X):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bci/lib/python3.12/site-packages/sklearn/utils/validation.py:2954\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2952\u001b[39m         out = X, y\n\u001b[32m   2953\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2954\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2955\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2956\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bci/lib/python3.12/site-packages/sklearn/utils/validation.py:1105\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1099\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1100\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray.ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1101\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m while dim <= 2 is required\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1102\u001b[39m     )\n\u001b[32m   1104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1114\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bci/lib/python3.12/site-packages/sklearn/utils/validation.py:116\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;66;03m# First try an O(n) time, O(1) space solution for the common case that\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# everything is finite; fall back to O(n) space `np.isinf/isnan` or custom\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# Cython implementation to prevent false positives and provide a detailed\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[38;5;66;03m# error message.\u001b[39;00m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m np.errstate(over=\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     first_pass_isfinite = xp.isfinite(\u001b[43mxp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bci/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:2466\u001b[39m, in \u001b[36msum\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, initial, where)\u001b[39m\n\u001b[32m   2463\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m   2464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[32m-> \u001b[39m\u001b[32m2466\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2467\u001b[39m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msum\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2468\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\n\u001b[32m   2469\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bci/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:86\u001b[39m, in \u001b[36m_wrapreduction\u001b[39m\u001b[34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[39m\n\u001b[32m     83\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     84\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis=axis, out=out, **passkwargs)\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if folds_exist(fold_data_path, n_folds):\n",
    "    print(\"✅ All folds found, reusing existing DataModule...\")\n",
    "    dm = AlignedMicroDataModule(\n",
    "        data, align_labels, align_labels, pool_data, AlignCCA,\n",
    "        batch_size=batch_size, folds=n_folds, val_size=val_size,\n",
    "        augmentations=augmentations, data_path=fold_data_path\n",
    "    )\n",
    "else:\n",
    "    print(\"⚙️ Generating new folds...\")\n",
    "    dm = AlignedMicroDataModule(\n",
    "        data, align_labels, align_labels, pool_data, AlignCCA,\n",
    "        batch_size=batch_size, folds=n_folds, val_size=val_size,\n",
    "        augmentations=augmentations, data_path=fold_data_path\n",
    "    )\n",
    "    dm.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802387c6",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba70dfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gclip_val = 0.5\n",
    "fs = 200\n",
    "in_channels = data.shape[-1] # 111\n",
    "num_classes = 9\n",
    "kernel_time = 50  # ms\n",
    "kernel_size = int(kernel_time * fs / 1000)  # kernel length in samples\n",
    "stride_time = 50  # ms\n",
    "stride = int(stride_time * fs / 1000)  # stride length in samples\n",
    "padding = 0\n",
    "n_enc_layers = 2\n",
    "n_dec_layers = 1\n",
    "activ = False\n",
    "model_type = 'gru'\n",
    "\n",
    "max_epochs = 500\n",
    "log_dir = os.path.join(\"..\", \"data\", \"transformer_logs\")\n",
    "context_prefix = \"pooled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "587c8a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters = 100\n",
    "hidden_size = 500\n",
    "cnn_dropout = 0.3\n",
    "rnn_dropout = 0.3\n",
    "learning_rate = 1e-4\n",
    "l2_reg = 1e-5\n",
    "n_iters = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47ce5c0",
   "metadata": {},
   "source": [
    "**Model Architecture:**\n",
    "\n",
    "1. Temporal Conv:\n",
    "    - Input: 200 timesteps * 111 features\n",
    "    - Output: 20 timesteps * 100 features\n",
    "    - kernel (10 timesteps * 111 features) * 100 different versions of kernel\n",
    "2. Encoder:\n",
    "    - Input: 20 tokens * 100-dim vector per token\n",
    "    - Output: h_20, a 500-dim vector\n",
    "    - h_t = GRU(h_t-1, x_t)\n",
    "        * t = 20 tokens\n",
    "        * x_t = one token of 100-dim (features representing the current chunk of time)\n",
    "        * h_t = hidden state of 500-dim (a new set of features to represent the current + prev chunk of time)\n",
    "3. Decoder:\n",
    "    - Input: encoder's output, a 500-dim vector encoding the whole sequence\n",
    "    - Output: y_1, y_2, y_3, 3 classification results\n",
    "    - h_t = GRU(h_t-1, x_t)\n",
    "        * t = 3, each producing 1 phoneme prediction\n",
    "        * h_t = hidden state of 500-dim (features representing current chunk of time + previous phon prediction)\n",
    "        * x_t = a 500-dim representation of the predicted phon (x_0 = a sequence for class 0 \"SOS\")\n",
    "    - y_t = integer in 0-8 = a_simple_proj(h_t)\n",
    "    - x_t+1 = Embedding(y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bdba268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(data_path = '../data/pt_decoding_data_S62.pkl', \n",
    "              fold_data_path = '../data/training_data_pooled', \n",
    "              patient_id = 'S14', \n",
    "              batch_size = 500, \n",
    "              n_folds = 20, \n",
    "              val_size = 0.1):\n",
    "    \n",
    "    data_filename = os.path.expanduser(data_path)\n",
    "    pt_data = utils.load_pkl(data_filename)\n",
    "\n",
    "    phoneme_index = 1 # All samples center around phoneme 1 onset.\n",
    "    lab_type = 'phon'\n",
    "    algn_type = 'phon_seq'\n",
    "    tar_data, pre_data = utils.decoding_data_from_dict(pt_data, patient_id, phoneme_index,\n",
    "                                                    lab_type=lab_type,\n",
    "                                                    algn_type=algn_type)\n",
    "\n",
    "    augmentations = [augs.time_shifting, augs.noise_jitter, augs.scaling]\n",
    "    data = torch.Tensor(tar_data[0])\n",
    "    labels = torch.Tensor(tar_data[1]).long().unsqueeze(1) - 1 \n",
    "    align_labels = torch.Tensor(tar_data[2]).long() - 1\n",
    "    pool_data = [(torch.Tensor(p[0]), torch.Tensor(p[2]).long() - 1, torch.Tensor(p[2]).long() - 1) for p in pre_data]\n",
    "\n",
    "    if folds_exist(fold_data_path, n_folds):\n",
    "        print(\"✅ All folds found, reusing existing DataModule...\")\n",
    "        dm = AlignedMicroDataModule(\n",
    "            data, align_labels, align_labels, pool_data, AlignCCA,\n",
    "            batch_size=batch_size, folds=n_folds, val_size=val_size,\n",
    "            augmentations=augmentations, data_path=fold_data_path\n",
    "        )\n",
    "    else:\n",
    "        print(\"⚙️ Generating new folds...\")\n",
    "        dm = AlignedMicroDataModule(\n",
    "            data, align_labels, align_labels, pool_data, AlignCCA,\n",
    "            batch_size=batch_size, folds=n_folds, val_size=val_size,\n",
    "            augmentations=augmentations, data_path=fold_data_path\n",
    "        )\n",
    "        dm.setup()\n",
    "    \n",
    "    torch.save(\n",
    "        {\"patient_id\": patient_id, \n",
    "         \"n_folds\": n_folds, \n",
    "         \"dm\": dm, \n",
    "         \"data\": torch.Tensor(tar_data[0])}, \n",
    "         f\"{fold_data_path}/{patient_id}_prep.pt\")\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9c3ebd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Current working directory: /Users/wangmaidou/Documents/EEG大模型/BCI Code/seq2seq_RNN/nn_models\n",
      "DEBUG: data_path: ../data/pt_decoding_data_S62.pkl\n",
      "DEBUG: fold_data_path: ../data/training_data_pooled\n",
      "DEBUG: patient_id: S14\n",
      "DEBUG: data_filename: ../data/pt_decoding_data_S62.pkl\n",
      "DEBUG: File exists: True\n",
      "❌ Missing: ../data/training_data_pooled/fold_data/fold_0.h5\n",
      "⚙️ Generating new folds...\n",
      "DEBUG labels.shape: torch.Size([144, 3])\n",
      "DEBUG first 5 rows of labels:\n",
      " tensor([[8, 0, 4],\n",
      "        [7, 1, 4],\n",
      "        [2, 5, 3],\n",
      "        [1, 7, 0],\n",
      "        [1, 8, 0]])\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/wangmaidou/Documents/EEG大模型/BCI Code/seq2seq_RNN/nn_models/CLI.py\", line 190, in <module>\n",
      "    fire.Fire({\n",
      "  File \"/opt/anaconda3/envs/bci/lib/python3.12/site-packages/fire/core.py\", line 135, in Fire\n",
      "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/bci/lib/python3.12/site-packages/fire/core.py\", line 468, in _Fire\n",
      "    component, remaining_args = _CallAndUpdateTrace(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/bci/lib/python3.12/site-packages/fire/core.py\", line 684, in _CallAndUpdateTrace\n",
      "    component = fn(*varargs, **kwargs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/wangmaidou/Documents/EEG大模型/BCI Code/seq2seq_RNN/nn_models/CLI.py\", line 64, in data_prep\n",
      "    dm.setup()\n",
      "  File \"/Users/wangmaidou/Documents/EEG大模型/BCI Code/seq2seq_RNN/nn_models/data_utils/datamodules.py\", line 253, in setup\n",
      "    process_aligner(aug_data, aug_labels, aug_align_labels,\n",
      "  File \"/Users/wangmaidou/Documents/EEG大模型/BCI Code/seq2seq_RNN/nn_models/data_utils/datamodules.py\", line 488, in process_aligner\n",
      "    X_pool = np.vstack([X_tar_dr] + X_algn_dr)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/bci/lib/python3.12/site-packages/numpy/_core/shape_base.py\", line 292, in vstack\n",
      "    return _nx.concatenate(arrs, 0, dtype=dtype, casting=casting)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python CLI.py data_prep --data_path=\"../data/pt_decoding_data_S62.pkl\" --fold_data_path=\"../data/training_data_pooled\" --n_folds=20 --batch_size=32 --val_size=0.2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1eedd71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Missing: ../data/training_data_pooled/fold_data/fold_7.h5\n",
      "⚙️ Generating new folds...\n",
      "DEBUG labels.shape: torch.Size([144, 3])\n",
      "DEBUG first 5 rows of labels:\n",
      " tensor([[8, 0, 4],\n",
      "        [7, 1, 4],\n",
      "        [2, 5, 3],\n",
      "        [1, 7, 0],\n",
      "        [1, 8, 0]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdata_prep\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../data/pt_decoding_data_S62.pkl\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../data/training_data_pooled\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mS14\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mdata_prep\u001b[39m\u001b[34m(data_path, fold_data_path, patient_id, batch_size, n_folds, val_size)\u001b[39m\n\u001b[32m     32\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m⚙️ Generating new folds...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m     dm = AlignedMicroDataModule(\n\u001b[32m     34\u001b[39m         data, align_labels, align_labels, pool_data, AlignCCA,\n\u001b[32m     35\u001b[39m         batch_size=batch_size, folds=n_folds, val_size=val_size,\n\u001b[32m     36\u001b[39m         augmentations=augmentations, data_path=fold_data_path\n\u001b[32m     37\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[43mdm\u001b[49m\u001b[43m.\u001b[49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m torch.save(\n\u001b[32m     41\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mpatient_id\u001b[39m\u001b[33m\"\u001b[39m: patient_id, \n\u001b[32m     42\u001b[39m      \u001b[33m\"\u001b[39m\u001b[33mn_folds\u001b[39m\u001b[33m\"\u001b[39m: n_folds, \n\u001b[32m     43\u001b[39m      \u001b[33m\"\u001b[39m\u001b[33mdm\u001b[39m\u001b[33m\"\u001b[39m: dm, \n\u001b[32m     44\u001b[39m      \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m: torch.Tensor(tar_data[\u001b[32m0\u001b[39m])}, \n\u001b[32m     45\u001b[39m      \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_data_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpatient_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_prep.pt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/EEG大模型/BCI Code/seq2seq_RNN/nn_models/data_utils/datamodules.py:253\u001b[39m, in \u001b[36mAlignedMicroDataModule.setup\u001b[39m\u001b[34m(self, stage)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m train_data, train_labels, align_labels\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# align pooled data to current data\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;66;03m# train_data, train_labels, dim_red = (\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m#     process_aligner(train_data, train_labels, align_labels,\u001b[39;00m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m#                     self.pool_data, self.algner))\u001b[39;00m\n\u001b[32m    252\u001b[39m aug_data, aug_labels, dim_red = (\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[43mprocess_aligner\u001b[49m\u001b[43m(\u001b[49m\u001b[43maug_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maug_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maug_align_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m                    \u001b[49m\u001b[43maug_pool_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43malgner\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    256\u001b[39m \u001b[38;5;66;03m# aug_data = torch.Tensor([])\u001b[39;00m\n\u001b[32m    257\u001b[39m \u001b[38;5;66;03m# aug_labels = torch.Tensor([]).long()\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# for aug in self.augmentations:\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m#     aug_data = torch.cat((aug_data, aug(train_data)))\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;66;03m#     aug_labels = torch.cat((aug_labels, train_labels))\u001b[39;00m\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m val_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/EEG大模型/BCI Code/seq2seq_RNN/nn_models/data_utils/datamodules.py:481\u001b[39m, in \u001b[36mprocess_aligner\u001b[39m\u001b[34m(X, y, y_align, pool_data, algner, n_components)\u001b[39m\n\u001b[32m    479\u001b[39m X_algn_dr = []\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, algn \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(aligns):\n\u001b[32m--> \u001b[39m\u001b[32m481\u001b[39m     \u001b[43malgn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tar_dr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_cross_dr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_align\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_align_cross\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    482\u001b[39m     X_algn_dr.append(algn.transform(X_cross_dr[i]))\n\u001b[32m    484\u001b[39m \u001b[38;5;66;03m# X_algn_dr = [x.reshape(x.shape[0], -1) for x in X_algn_dr]\u001b[39;00m\n\u001b[32m    485\u001b[39m \u001b[38;5;66;03m# X_tar_dr = X_tar_dr.reshape(X_tar_dr.shape[0], -1)\u001b[39;00m\n\u001b[32m    486\u001b[39m \n\u001b[32m    487\u001b[39m \u001b[38;5;66;03m# concatenate cross-patient data\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/EEG大模型/BCI Code/seq2seq_RNN/nn_models/../alignment/AlignCCA.py:18\u001b[39m, in \u001b[36mAlignCCA.fit\u001b[39m\u001b[34m(self, X_a, X_b, y_a, y_b)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X_a, X_b, y_a, y_b):\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     L_a, L_b = \u001b[43mreshape_latent_dynamics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     M_a, M_b, S = CCA_align(L_a.T, L_b.T)\n\u001b[32m     20\u001b[39m     \u001b[38;5;28mself\u001b[39m.M_a = M_a\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/EEG大模型/BCI Code/seq2seq_RNN/nn_models/../alignment/AlignCCA.py:55\u001b[39m, in \u001b[36mreshape_latent_dynamics\u001b[39m\u001b[34m(X_a, X_b, y_a, y_b, type)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreshape_latent_dynamics\u001b[39m(X_a, X_b, y_a, y_b, \u001b[38;5;28mtype\u001b[39m=\u001b[33m'\u001b[39m\u001b[33mclass\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m == \u001b[33m'\u001b[39m\u001b[33mclass\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m         L_a, L_b = \u001b[43mextract_latent_dynamics_by_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_b\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m == \u001b[33m'\u001b[39m\u001b[33mtrial\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     57\u001b[39m         L_a, L_b = extract_latent_dynamics_by_trial_subselect(X_a, X_b, y_a,\n\u001b[32m     58\u001b[39m                                                               y_b)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/EEG大模型/BCI Code/seq2seq_RNN/nn_models/../alignment/AlignCCA.py:71\u001b[39m, in \u001b[36mextract_latent_dynamics_by_class\u001b[39m\u001b[34m(X_a, X_b, y_a, y_b)\u001b[39m\n\u001b[32m     69\u001b[39m y_a, y_b = label2str(y_a), label2str(y_b)\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# average trials within class\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m L_a, L_b = cnd_avg(X_a, y_a), \u001b[43mcnd_avg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_b\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# only align via shared classes between datasets\u001b[39;00m\n\u001b[32m     74\u001b[39m _, y_shared_a, y_shared_b = np.intersect1d(np.unique(y_a), np.unique(y_b),\n\u001b[32m     75\u001b[39m                                            assume_unique=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     76\u001b[39m                                            return_indices=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/EEG大模型/BCI Code/seq2seq_RNN/nn_models/../alignment/alignment_utils.py:47\u001b[39m, in \u001b[36mcnd_avg\u001b[39m\u001b[34m(data, labels)\u001b[39m\n\u001b[32m     45\u001b[39m data_by_class = np.zeros(class_shape)\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, seq \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(np.unique(labels)):\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     data_by_class[i] = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data_by_class\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bci/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:3730\u001b[39m, in \u001b[36m_mean_dispatcher\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, where)\u001b[39m\n\u001b[32m   3715\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3716\u001b[39m \u001b[33;03m    Round an array to the given number of decimals.\u001b[39;00m\n\u001b[32m   3717\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3725\u001b[39m \n\u001b[32m   3726\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   3727\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[33m'\u001b[39m\u001b[33mround\u001b[39m\u001b[33m'\u001b[39m, decimals=decimals, out=out)\n\u001b[32m-> \u001b[39m\u001b[32m3730\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_mean_dispatcher\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mNone\u001b[39;00m, *,\n\u001b[32m   3731\u001b[39m                      where=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   3732\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, where, out)\n\u001b[32m   3735\u001b[39m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_mean_dispatcher)\n\u001b[32m   3736\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmean\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=np._NoValue, *,\n\u001b[32m   3737\u001b[39m          where=np._NoValue):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "data_prep('../data/pt_decoding_data_S62.pkl', '../data/training_data_pooled', 'S14', 500, 20, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38b8e17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Setting up data module for iteration 1 #####\n",
      "DEBUG labels.shape: torch.Size([144, 3])\n",
      "DEBUG first 5 rows of labels:\n",
      " tensor([[8, 0, 4],\n",
      "        [7, 1, 4],\n",
      "        [2, 5, 3],\n",
      "        [1, 7, 0],\n",
      "        [1, 8, 0]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_iters):\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m##### Setting up data module for iteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m #####\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[43mdm\u001b[49m\u001b[43m.\u001b[49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# prepare dataset (splits, etc.)\u001b[39;00m\n\u001b[32m      9\u001b[39m     fold_accs = []  \u001b[38;5;66;03m# store results for this iteration’s folds\u001b[39;00m\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# go through each fold\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/EEG大模型/BCI Code/seq2seq_RNN/nn_models/data_utils/datamodules.py:253\u001b[39m, in \u001b[36mAlignedMicroDataModule.setup\u001b[39m\u001b[34m(self, stage)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m train_data, train_labels, align_labels\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# align pooled data to current data\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;66;03m# train_data, train_labels, dim_red = (\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m#     process_aligner(train_data, train_labels, align_labels,\u001b[39;00m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m#                     self.pool_data, self.algner))\u001b[39;00m\n\u001b[32m    252\u001b[39m aug_data, aug_labels, dim_red = (\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[43mprocess_aligner\u001b[49m\u001b[43m(\u001b[49m\u001b[43maug_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maug_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maug_align_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m                    \u001b[49m\u001b[43maug_pool_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43malgner\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    256\u001b[39m \u001b[38;5;66;03m# aug_data = torch.Tensor([])\u001b[39;00m\n\u001b[32m    257\u001b[39m \u001b[38;5;66;03m# aug_labels = torch.Tensor([]).long()\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# for aug in self.augmentations:\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m#     aug_data = torch.cat((aug_data, aug(train_data)))\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;66;03m#     aug_labels = torch.cat((aug_labels, train_labels))\u001b[39;00m\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m val_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/EEG大模型/BCI Code/seq2seq_RNN/nn_models/data_utils/datamodules.py:459\u001b[39m, in \u001b[36mprocess_aligner\u001b[39m\u001b[34m(X, y, y_align, pool_data, algner, n_components)\u001b[39m\n\u001b[32m    456\u001b[39m X_tar_r = X.reshape(-\u001b[32m1\u001b[39m, X.shape[-\u001b[32m1\u001b[39m])\n\u001b[32m    458\u001b[39m \u001b[38;5;66;03m# reduce dimensionality of cross-patient data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m X_cross_dr = [\u001b[43mPCA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    460\u001b[39m               \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X_cross_r]\n\u001b[32m    462\u001b[39m \u001b[38;5;66;03m# reduce dimensionality of target data, saving dim. red. object for\u001b[39;00m\n\u001b[32m    463\u001b[39m \u001b[38;5;66;03m# test set\u001b[39;00m\n\u001b[32m    464\u001b[39m tar_dr = PCA(n_components=n_components)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bci/lib/python3.12/site-packages/sklearn/utils/_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bci/lib/python3.12/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bci/lib/python3.12/site-packages/sklearn/decomposition/_pca.py:466\u001b[39m, in \u001b[36mPCA.fit_transform\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    443\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    444\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    445\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[32m    446\u001b[39m \n\u001b[32m    447\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    464\u001b[39m \u001b[33;03m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[32m    465\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m     U, S, _, X, x_is_centered, xp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    467\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m U \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    468\u001b[39m         U = U[:, : \u001b[38;5;28mself\u001b[39m.n_components_]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bci/lib/python3.12/site-packages/sklearn/decomposition/_pca.py:503\u001b[39m, in \u001b[36mPCA._fit\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    493\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    494\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPCA with svd_solver=\u001b[39m\u001b[33m'\u001b[39m\u001b[33marpack\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is not supported for Array API inputs.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    495\u001b[39m     )\n\u001b[32m    497\u001b[39m \u001b[38;5;66;03m# Validate the data, without ever forcing a copy as any solver that\u001b[39;00m\n\u001b[32m    498\u001b[39m \u001b[38;5;66;03m# supports sparse input data and the `covariance_eigh` solver are\u001b[39;00m\n\u001b[32m    499\u001b[39m \u001b[38;5;66;03m# written in a way to avoid the need for any inplace modification of\u001b[39;00m\n\u001b[32m    500\u001b[39m \u001b[38;5;66;03m# the input data contrary to the other solvers.\u001b[39;00m\n\u001b[32m    501\u001b[39m \u001b[38;5;66;03m# The copy will happen\u001b[39;00m\n\u001b[32m    502\u001b[39m \u001b[38;5;66;03m# later, only if needed, once the solver negotiation below is done.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[38;5;28mself\u001b[39m._fit_svd_solver = \u001b[38;5;28mself\u001b[39m.svd_solver\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fit_svd_solver == \u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m issparse(X):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bci/lib/python3.12/site-packages/sklearn/utils/validation.py:2954\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2952\u001b[39m         out = X, y\n\u001b[32m   2953\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2954\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2955\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2956\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bci/lib/python3.12/site-packages/sklearn/utils/validation.py:1053\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1051\u001b[39m         array = xp.astype(array, dtype, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1052\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1053\u001b[39m         array = \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1054\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[32m   1055\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1056\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.format(array)\n\u001b[32m   1057\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcomplex_warning\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bci/lib/python3.12/site-packages/sklearn/utils/_array_api.py:757\u001b[39m, in \u001b[36m_asarray_with_order\u001b[39m\u001b[34m(array, dtype, order, copy, xp, device)\u001b[39m\n\u001b[32m    755\u001b[39m     array = numpy.array(array, order=order, dtype=dtype)\n\u001b[32m    756\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m     array = \u001b[43mnumpy\u001b[49m\u001b[43m.\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    759\u001b[39m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[32m    760\u001b[39m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[32m    761\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m xp.asarray(array)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/bci/lib/python3.12/site-packages/torch/_tensor.py:1151\u001b[39m, in \u001b[36mTensor.__array__\u001b[39m\u001b[34m(self, dtype)\u001b[39m\n\u001b[32m   1149\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.numpy()\n\u001b[32m   1150\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1151\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# specify a folder to save all accuracy data\n",
    "acc_dir = os.path.join(\"..\", \"data\", \"accuracy_data_post_training\")\n",
    "iter_accs = []  # store all accuracies across iterations\n",
    "\n",
    "for i in range(n_iters):\n",
    "    print(f'##### Setting up data module for iteration {i+1} #####')\n",
    "    dm.setup()  # prepare dataset (splits, etc.)\n",
    "    \n",
    "    fold_accs = []  # store results for this iteration’s folds\n",
    "\n",
    "    # go through each fold\n",
    "    for fold in range(n_folds):\n",
    "        dm.set_fold(fold)\n",
    "        in_channels = dm.get_data_shape()[-1]  # number of input features (111 channels)\n",
    "\n",
    "        # build a fresh Seq2SeqRNN for this fold\n",
    "        model = Seq2SeqRNN(in_channels, n_filters, hidden_size, num_classes, n_enc_layers,\n",
    "                           n_dec_layers, kernel_size, stride, padding, cnn_dropout, rnn_dropout, model_type,\n",
    "                           learning_rate, l2_reg, activation=activ, decay_iters=max_epochs)\n",
    "\n",
    "        # callbacks: save best model + log learning rate\n",
    "        callbacks = [\n",
    "            ModelCheckpoint(monitor='val_acc', mode='max'),\n",
    "            LearningRateMonitor(logging_interval='epoch'),\n",
    "        ]\n",
    "\n",
    "        # trainer controls training loop\n",
    "        trainer = L.Trainer(default_root_dir=log_dir,\n",
    "                            max_epochs=max_epochs,\n",
    "                            gradient_clip_val=gclip_val,   # prevent exploding gradients\n",
    "                            accelerator='auto',            # GPU if available\n",
    "                            callbacks=callbacks,\n",
    "                            logger=True,\n",
    "                            enable_model_summary=False,\n",
    "                            enable_progress_bar=False)\n",
    "\n",
    "        # train and validate for all epochs\n",
    "        trainer.fit(model=model, \n",
    "                    train_dataloaders=dm.train_dataloader(), \n",
    "                    val_dataloaders=dm.val_dataloader())\n",
    "\n",
    "        # print training/validation metrics\n",
    "        print(trainer.logged_metrics)\n",
    "\n",
    "        # take the epoch with the best validation checkpoint, test it\n",
    "        trainer.test(model=model, \n",
    "                     dataloaders=dm.test_dataloader(), \n",
    "                     ckpt_path='best')\n",
    "\n",
    "        # store test accuracy for this fold (just one number)\n",
    "        fold_accs.append(trainer.logged_metrics['test_acc'])\n",
    "    \n",
    "    # store fold accuracies for this iteration (n_fold numbers)\n",
    "    iter_accs.append(fold_accs)\n",
    "\n",
    "    # ensure save folder exists\n",
    "    os.makedirs(os.path.join(acc_dir, context_prefix), exist_ok=True)\n",
    "\n",
    "    # save partial results (up to this iteration) as CSV\n",
    "    with open(os.path.join(acc_dir, f\"{context_prefix}/{patient_id}_{context_prefix}_seq2seq_rnn_accs_iter{i+1}.csv\"), 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(iter_accs)\n",
    "\n",
    "    # print average accuracy across folds for this iteration\n",
    "    print(np.mean(fold_accs))\n",
    "\n",
    "# after all iterations: save final results\n",
    "print(iter_accs)\n",
    "\n",
    "# save all accuracies into one CSV\n",
    "with open(os.path.join(acc_dir, f\"{context_prefix}/{patient_id}_{context_prefix}_seq2seq_rnn_accs.csv\"), 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(iter_accs)\n",
    "\n",
    "# also save as .npy (easy reload in Python)\n",
    "np.save(os.path.join(acc_dir, f\"{context_prefix}/{patient_id}_{context_prefix}_seq2seq_rnn_accs.npy\"),\n",
    "        np.array(iter_accs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
